import logging
import numpy as np
from typing import List, Dict, Any, Tuple, Optional, Union
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge import Rouge
from bert_score import score as bert_score
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Download NLTK data
try:
    nltk.download('punkt', quiet=True)
except:
    pass

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GenerationEvaluator:
    """
    Class for evaluating the quality of generated responses.
    """
    
    def __init__(self):
        """
        Initialize the generation evaluator.
        """
        self.rouge = Rouge()
        self.smoothing = SmoothingFunction().method1
    
    def evaluate_response(
        self, 
        generated_response: str, 
        reference_response: str
    ) -> Dict[str, float]:
        """
        Evaluate a generated response against a reference response.
        
        Args:
            generated_response: The response generated by the model
            reference_response: The reference (ground truth) response
        
        Returns:
            Dictionary of evaluation metrics
        """
        logger.info("Evaluating generated response quality")
        
        # Tokenize responses
        gen_tokens = nltk.word_tokenize(generated_response.lower())
        ref_tokens = nltk.word_tokenize(reference_response.lower())
        
        # Calculate BLEU score
        try:
            bleu_score = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=self.smoothing)
        except Exception as e:
            logger.warning(f"Error calculating BLEU score: {str(e)}")
            bleu_score = 0
        
        # Calculate ROUGE scores
        try:
            rouge_scores = self.rouge.get_scores(generated_response, reference_response)[0]
        except Exception as e:
            logger.warning(f"Error calculating ROUGE scores: {str(e)}")
            rouge_scores = {
                "rouge-1": {"f": 0, "p": 0, "r": 0},
                "rouge-2": {"f": 0, "p": 0, "r": 0},
                "rouge-l": {"f": 0, "p": 0, "r": 0}
            }
        
        # Calculate BERTScore
        try:
            precision, recall, f1 = bert_score([generated_response], [reference_response], lang="en")
            bert_f1 = f1.item()
        except Exception as e:
            logger.warning(f"Error calculating BERTScore: {str(e)}")
            bert_f1 = 0
        
        # Compile metrics
        metrics = {
            "bleu": bleu_score,
            "rouge1_f": rouge_scores["rouge-1"]["f"],
            "rouge2_f": rouge_scores["rouge-2"]["f"],
            "rougeL_f": rouge_scores["rouge-l"]["f"],
            "bert_score": bert_f1
        }
        
        logger.info(f"Generation evaluation metrics: {metrics}")
        return metrics
    
    def evaluate_batch(
        self, 
        generated_responses: List[str], 
        reference_responses: List[str]
    ) -> Dict[str, Any]:
        """
        Evaluate multiple generated responses against reference responses.
        
        Args:
            generated_responses: List of responses generated by the model
            reference_responses: List of reference (ground truth) responses
        
        Returns:
            Dictionary of aggregated evaluation metrics and per-response results
        """
        if len(generated_responses) != len(reference_responses):
            logger.error("Number of generated responses does not match number of reference responses")
            return {}
        
        logger.info(f"Evaluating batch of {len(generated_responses)} responses")
        
        results = []
        
        for gen, ref in zip(generated_responses, reference_responses):
            # Evaluate this response pair
            metrics = self.evaluate_response(gen, ref)
            
            # Store results
            results.append({
                "generated": gen,
                "reference": ref,
                "metrics": metrics
            })
        
        # Calculate aggregate metrics
        aggregated = {
            "bleu": np.mean([r["metrics"]["bleu"] for r in results]),
            "rouge1_f": np.mean([r["metrics"]["rouge1_f"] for r in results]),
            "rouge2_f": np.mean([r["metrics"]["rouge2_f"] for r in results]),
            "rougeL_f": np.mean([r["metrics"]["rougeL_f"] for r in results]),
            "bert_score": np.mean([r["metrics"]["bert_score"] for r in results]),
            "num_responses": len(results),
            "per_response_results": results
        }
        
        logger.info(f"Aggregate evaluation metrics: bleu={aggregated['bleu']:.3f}, rouge1_f={aggregated['rouge1_f']:.3f}, bert_score={aggregated['bert_score']:.3f}")
        return aggregated
    
    def visualize_evaluation_results(self, results: Dict[str, Any]) -> plt.Figure:
        """
        Visualize generation evaluation results across multiple metrics.
        
        Args:
            results: Aggregated evaluation results
        
        Returns:
            Matplotlib figure with evaluation visualization
        """
        # Extract per-response metrics
        if "per_response_results" not in results:
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, "No detailed results available", ha='center', va='center')
            return fig
            
        response_results = results["per_response_results"]
        
        # Create a DataFrame for easy plotting
        data = []
        for i, res in enumerate(response_results):
            data.append({
                "response_id": f"Response {i+1}",
                "bleu": res["metrics"]["bleu"],
                "rouge1_f": res["metrics"]["rouge1_f"],
                "rouge2_f": res["metrics"]["rouge2_f"],
                "rougeL_f": res["metrics"]["rougeL_f"],
                "bert_score": res["metrics"]["bert_score"]
            })
            
        df = pd.DataFrame(data)
        
        # Melt the DataFrame for seaborn plotting
        df_melted = pd.melt(df, id_vars=['response_id'], var_name='Metric', value_name='Score')
        
        # Create figure
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Plot heatmap
        pivot_table = df_melted.pivot(index='response_id', columns='Metric', values='Score')
        sns.heatmap(pivot_table, annot=True, cmap='viridis', linewidths=0.5, ax=ax)
        
        # Add title and labels
        ax.set_title('Generation Evaluation Results by Response and Metric', fontsize=16)
        ax.set_xlabel('Evaluation Metric', fontsize=12)
        ax.set_ylabel('Response', fontsize=12)
        
        fig.tight_layout()
        return fig
    
    def human_evaluation_form(self) -> Dict[str, Any]:
        """
        Create a template for human evaluation of generated responses.
        
        Returns:
            Dictionary with human evaluation criteria and instructions
        """
        form = {
            "instructions": """
            Please evaluate the generated response on the following criteria:
            - Relevance: How well does the response address the query?
            - Accuracy: How factually accurate is the information in the response?
            - Completeness: How complete is the response in addressing all aspects of the query?
            - Coherence: How well-structured and logically coherent is the response?
            - Helpfulness: How helpful would this response be to a user?
            
            Rate each criterion on a scale of 1-5, where:
            1 = Poor
            2 = Fair
            3 = Good
            4 = Very Good
            5 = Excellent
            """,
            
            "criteria": [
                {"name": "Relevance", "description": "Response directly addresses the query", "score": None},
                {"name": "Accuracy", "description": "Information is factually correct", "score": None},
                {"name": "Completeness", "description": "All aspects of the query are addressed", "score": None},
                {"name": "Coherence", "description": "Response is well-structured and logical", "score": None},
                {"name": "Helpfulness", "description": "Response would be helpful to the user", "score": None}
            ],
            
            "additional_feedback": "",
            
            "query": "",
            "response": ""
        }
        
        return form
    
    def analyze_human_evaluations(self, evaluations: List[Dict[str, Any]]) -> Tuple[Dict[str, float], plt.Figure]:
        """
        Analyze human evaluations of generated responses.
        
        Args:
            evaluations: List of completed human evaluation forms
        
        Returns:
            Tuple of (aggregated metrics, visualization figure)
        """
        if not evaluations:
            logger.warning("No human evaluations to analyze")
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, "No human evaluation data available", ha='center', va='center')
            return {}, fig
        
        logger.info(f"Analyzing {len(evaluations)} human evaluations")
        
        # Extract scores for each criterion
        criteria = evaluations[0]["criteria"]
        criterion_names = [c["name"] for c in criteria]
        
        scores_by_criterion = {name: [] for name in criterion_names}
        
        for eval_form in evaluations:
            for criterion in eval_form["criteria"]:
                name = criterion["name"]
                score = criterion["score"]
                if score is not None:
                    scores_by_criterion[name].append(score)
        
        # Calculate average scores
        avg_scores = {}
        for name, scores in scores_by_criterion.items():
            if scores:
                avg_scores[name] = np.mean(scores)
            else:
                avg_scores[name] = 0
        
        # Create visualization
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # Bar chart of average scores
        names = list(avg_scores.keys())
        values = list(avg_scores.values())
        
        ax1.bar(names, values, color='skyblue')
        ax1.set_title('Average Scores by Criterion', fontsize=14)
        ax1.set_ylim([0, 5])
        ax1.set_ylabel('Average Score')
        ax1.tick_params(axis='x', rotation=45)
        
        # Add value labels on top of bars
        for i, v in enumerate(values):
            ax1.text(i, v + 0.1, f'{v:.2f}', ha='center')
        
        # Boxplot of score distributions
        data_for_boxplot = [scores_by_criterion[name] for name in names]
        ax2.boxplot(data_for_boxplot, labels=names)
        ax2.set_title('Score Distributions by Criterion', fontsize=14)
        ax2.set_ylim([0, 5.5])
        ax2.set_ylabel('Score')
        ax2.tick_params(axis='x', rotation=45)
        
        # Overall title
        fig.suptitle('Human Evaluation Results', fontsize=16)
        
        fig.tight_layout()
        
        return avg_scores, fig